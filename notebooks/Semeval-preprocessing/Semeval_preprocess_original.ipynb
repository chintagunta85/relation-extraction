{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the original preprocessing of semeval 2010 data. Just tokenization is performed. No stop word removal, no punctuation removal, no entity blinding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import os, random, pandas as pd, numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import ast\n",
    "sys.path.append('../../')\n",
    "# sys.path.append('../ddi_preprocess')\n",
    "from relation_extraction.data import utils\n",
    "import nltk\n",
    "from ast import literal_eval\n",
    "import itertools\n",
    "from ast import literal_eval # to convert the string tuple form to an actual tuple\n",
    "RESOURCE_PATH = \"/data/medg/misc/geeticka/relation_extraction/semeval_2010\"\n",
    "outdir = 'pre-processed/original/'\n",
    "def res(path): return os.path.join(RESOURCE_PATH, path)\n",
    "from relation_extraction.data.converters.converter_semeval2010 import get_dataset_dataframe, write_dataframe,\\\n",
    "read_dataframe, check_equality_of_written_and_read_df, rev_relation_dict, write_into_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = get_dataset_dataframe(res('TRAIN_FILE.TXT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = get_dataset_dataframe(res('TEST_FILE_FULL.TXT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2717"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lengths seem to be correct. Now we want to check if any entity is empty/ was not detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for empty entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_empty_entity_rows(df):\n",
    "    empty_entity_rows = []\n",
    "    def find_empty_entity_number(row):\n",
    "        metadata = row.metadata\n",
    "        e1 = metadata['e1']['word_index']\n",
    "        e2 = metadata['e2']['word_index']\n",
    "        if not e1 or not e2:\n",
    "            empty_entity_rows.append(row.row_num)\n",
    "    temp_df = df.copy()\n",
    "    temp_df.insert(0, 'row_num', range(0, len(temp_df)))\n",
    "    temp_df.apply(find_empty_entity_number, axis=1)\n",
    "    return empty_entity_rows\n",
    "\n",
    "def get_empty_rows_array(empty_entity_rows, df):\n",
    "    empty_rows_array = []\n",
    "    for index in empty_entity_rows:\n",
    "        e1 = df.iloc[index].e1\n",
    "        e2 = df.iloc[index].e2\n",
    "        original_sentence = df.iloc[index].original_sentence\n",
    "        tokenized_sentence = df.iloc[index].tokenized_sentence\n",
    "        metadata = df.iloc[index].metadata\n",
    "        empty_rows_array.append([index, original_sentence, e1, e2, metadata, tokenized_sentence])\n",
    "    new_df = pd.DataFrame(data=empty_rows_array,    # values\n",
    "             columns=['index_original', 'original_sentence' , 'e1', 'e2', 'metadata', 'tokenized_sentence'])\n",
    "    return empty_rows_array, new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_empty_vals(df):\n",
    "    empty_entity_rows = get_empty_entity_rows(df)\n",
    "    empty_rows_array, new_df = get_empty_rows_array(empty_entity_rows, df)\n",
    "    return empty_rows_array, new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], Empty DataFrame\n",
       " Columns: [index_original, original_sentence, e1, e2, metadata, tokenized_sentence]\n",
       " Index: [])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_empty_vals(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], Empty DataFrame\n",
       " Columns: [index_original, original_sentence, e1, e2, metadata, tokenized_sentence]\n",
       " Index: [])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_empty_vals(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write into the csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(res(outdir)):\n",
    "    os.makedirs(res(outdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframe(df_train, res(outdir + 'train_original.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = read_dataframe(res(outdir + 'train_original.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first checks with the pd.equals method, and the other does a manual checking per column\n",
    "check_equality_of_written_and_read_df(df_train, df_train_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframe(df_test, res(outdir + 'test_original.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_copy = read_dataframe(res(outdir + 'test_original.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_equality_of_written_and_read_df(df_test, df_test_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the files into txt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique relations: \t ['Component-Whole(e2,e1)' 'Other' 'Instrument-Agency(e2,e1)'\n",
      " 'Member-Collection(e1,e2)' 'Cause-Effect(e2,e1)'\n",
      " 'Entity-Destination(e1,e2)' 'Content-Container(e1,e2)'\n",
      " 'Message-Topic(e1,e2)' 'Product-Producer(e2,e1)'\n",
      " 'Member-Collection(e2,e1)' 'Entity-Origin(e1,e2)' 'Cause-Effect(e1,e2)'\n",
      " 'Component-Whole(e1,e2)' 'Message-Topic(e2,e1)' 'Product-Producer(e1,e2)'\n",
      " 'Entity-Origin(e2,e1)' 'Content-Container(e2,e1)'\n",
      " 'Instrument-Agency(e1,e2)' 'Entity-Destination(e2,e1)']\n"
     ]
    }
   ],
   "source": [
    "write_into_txt(df_train, res(outdir + 'train_original.txt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique relations: \t ['Message-Topic(e1,e2)' 'Product-Producer(e2,e1)'\n",
      " 'Instrument-Agency(e2,e1)' 'Entity-Destination(e1,e2)'\n",
      " 'Cause-Effect(e2,e1)' 'Component-Whole(e1,e2)' 'Product-Producer(e1,e2)'\n",
      " 'Member-Collection(e2,e1)' 'Other' 'Entity-Origin(e1,e2)'\n",
      " 'Content-Container(e1,e2)' 'Entity-Origin(e2,e1)' 'Cause-Effect(e1,e2)'\n",
      " 'Component-Whole(e2,e1)' 'Content-Container(e2,e1)'\n",
      " 'Instrument-Agency(e1,e2)' 'Message-Topic(e2,e1)'\n",
      " 'Member-Collection(e1,e2)' 'Entity-Destination(e2,e1)']\n"
     ]
    }
   ],
   "source": [
    "write_into_txt(df_test, res(outdir + 'test_original.txt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
