{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generates the train-drugbank_filtered.csv (DDI-Dataset-Preprocess generates train-drugbank.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given e1_pos, e2_pos, sentence. Update the sentence to say DRUG and OTHER_DRUG and update the positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'The bear_e1 ran home_e2 to_e2 the mountains'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /afs/csail.mit.edu/u/g/geeticka/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "import sys\n",
    "sys.path.append('../../../../')\n",
    "import ast\n",
    "import relation_extraction.data.utils as utils\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "RESOURCE_PATH = \"/data/medg/misc/semeval_2010/medical-data/DDICorpus/pre-processed/extraction/\"\n",
    "def res(path): return os.path.join(RESOURCE_PATH, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up rows where e1 doesn't happen before e2 and the two entities are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(res('Train/MedLine/train-medline.csv')) # where you read from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1546"
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems like when you read the csv file, the entity_number row becomes string instead of being a list of tuples,\n",
    "# so need to segment with regular expressions\n",
    "def get_problematic_entity_rows(df):\n",
    "    problematic_entity_rows = [] # those which have e's appearing beyond the sentence \n",
    "    flipped_entity_rows = [] # (automatic) non overlapping entities but e0 appears after e1, can be flipped and that's it \n",
    "    flipped_entity_rows_non_adjacent = [] # (semi-automatic) can be flipped and then should be manually fixed\n",
    "    overlapping_entity_rows = [] # (manual) overlapping entities, which needs to be manually fixed\n",
    "    same_entity_rows = [] # (automatic) entities which are exactly the same, needs to be deleted\n",
    "    non_adjacent_entity_rows = [] # (manual) entities which are not overlapping, but don't have continuous entity numbers, \n",
    "    # also needs to be manually fixed along with the sentence - could just let it be and not fix, but \n",
    "    # better to fix for accurate predictions. \n",
    "    def find_problematic_entity(row):\n",
    "        sentence_len = len(row.tokenized_sentence)\n",
    "        entity_number = ast.literal_eval(row.entity_number)\n",
    "        e1 = entity_number[0]\n",
    "        e2 = entity_number[1]\n",
    "        range_e1 = [i for i in range(e1[0], e1[-1] + 1)]\n",
    "        range_e2 = [i for i in range(e2[0], e2[-1] + 1)]\n",
    "        if e1[0] >= sentence_len or e2[0] >= sentence_len:\n",
    "            problematic_entity_rows.append(row.row_num)\n",
    "        elif e1[0] == e2[0] or e1[-1] == e2[-1]: # this could either be overlap or totally equal entity\n",
    "            if e1 == e2: # think about the asynchronous cases where 23, 25 is present\n",
    "                same_entity_rows.append(row.row_num)\n",
    "            else: # in this case there must be overlap\n",
    "                overlapping_entity_rows.append(row.row_num)\n",
    "        elif e1[0] > e2[0] or e1[-1] > e2[0]:\n",
    "            if not set(range_e1).intersection(set(range_e2)): # don't overlap in any way\n",
    "                if range_e1 != e1 or range_e2 != e2:\n",
    "                    flipped_entity_rows_non_adjacent.append(row.row_num)\n",
    "                else:\n",
    "                    flipped_entity_rows.append(row.row_num) # this could also \n",
    "                #have non adjacent entity rows, but let it be\n",
    "            else:\n",
    "                overlapping_entity_rows.append(row.row_num)\n",
    "        elif range_e1 != e1 or range_e2 != e2:\n",
    "            non_adjacent_entity_rows.append(row.row_num)\n",
    "    temp_df = df.copy()\n",
    "    temp_df.insert(0, 'row_num', range(0, len(temp_df)))\n",
    "    temp_df.apply(find_problematic_entity, axis=1)\n",
    "    return problematic_entity_rows, flipped_entity_rows, flipped_entity_rows_non_adjacent, \\\n",
    "overlapping_entity_rows, same_entity_rows, non_adjacent_entity_rows\n",
    "\n",
    "def get_problematic_entity_rows_array(problematic_entity_rows, df):\n",
    "    problematic_entity_rows_array = []\n",
    "    for index in problematic_entity_rows:\n",
    "        e1 = df.iloc[index].e1\n",
    "        e2 = df.iloc[index].e2\n",
    "        sentence_text = df.iloc[index].sentence_text\n",
    "        tokenized_sentence = df.iloc[index].tokenized_sentence\n",
    "        entity_number = df.iloc[index].entity_number\n",
    "        problematic_entity_rows_array.append([index, e1, e2, sentence_text, tokenized_sentence, entity_number])\n",
    "    new_df = pd.DataFrame(data=problematic_entity_rows_array,    # values\n",
    "             columns=['index_original', 'e1', 'e2', 'sentence_text', 'tokenized_sentence', 'entity_number'])\n",
    "    return problematic_entity_rows_array, new_df\n",
    "\n",
    "def get_problematic_entity_vals(df):\n",
    "    problematic_entity_rows, flipped_entity_rows, flipped_entity_rows_non_adjacent, \\\n",
    "    overlapping_entity_rows, same_entity_rows, non_adjacent_entity_rows = get_problematic_entity_rows(df)\n",
    "    _, df_problematic = get_problematic_entity_rows_array(problematic_entity_rows, df)\n",
    "    _, df_flipped = get_problematic_entity_rows_array(flipped_entity_rows, df)\n",
    "    _, df_flipped_non_adjacent = get_problematic_entity_rows_array(flipped_entity_rows_non_adjacent, df)\n",
    "    _, df_overlapping = get_problematic_entity_rows_array(overlapping_entity_rows, df)\n",
    "    _, df_same_entity = get_problematic_entity_rows_array(same_entity_rows, df)\n",
    "    _, df_non_adjacent = get_problematic_entity_rows_array(non_adjacent_entity_rows, df)\n",
    "    return df_problematic, df_flipped, df_flipped_non_adjacent, df_overlapping, df_same_entity, df_non_adjacent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_problematic, df_flipped, df_flipped_non_adjacent, df_overlapping, \\\n",
    "df_same_entity, df_non_adjacent = get_problematic_entity_vals(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure that df_problematic is length 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 114 0 8 154 3\n"
     ]
    }
   ],
   "source": [
    "print(len(df_problematic), len(df_flipped), len(df_flipped_non_adjacent), \n",
    "     len(df_overlapping), len(df_same_entity), len(df_non_adjacent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perform flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_entity_number(row):  \n",
    "    curr_entity_nums = ast.literal_eval(row.old_entity_number)\n",
    "    curr_e1_nums = curr_entity_nums[0]\n",
    "    curr_e2_nums = curr_entity_nums[1]\n",
    "    return curr_e2_nums, curr_e1_nums\n",
    "def fix_flipped(df_flipped):\n",
    "    new_df = df_flipped.copy()\n",
    "    new_df = new_df.rename(columns={'entity_number': 'old_entity_number'})\n",
    "    new_df['entity_number'] = new_df.apply(flip_entity_number, axis=1)\n",
    "    new_df = new_df.rename(columns={'e1': 'e2', 'e2':'e1'})\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for now, we are just going to delete the rows df_overlapping, df_same_entity, and we can keep df_non_adjacent because it probably doesn't hurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update based on entity number and e1 and e2 - no changes made to tokenized sentence\n",
    "def update_df_with_fixed_vals(fromdf, todf):\n",
    "    todf = todf.copy()\n",
    "    for i in range(0, len(fromdf)):\n",
    "        row = fromdf.iloc[i]\n",
    "        idx = row.index_original\n",
    "        entity_number = row.entity_number\n",
    "        e1 = row.e1\n",
    "        e2 = row.e2\n",
    "        todf.at[idx, 'entity_number'] = entity_number\n",
    "        todf.at[idx, 'e1'] = e1\n",
    "        todf.at[idx, 'e2'] = e2\n",
    "    return todf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_df = df.copy()\n",
    "if len(df_flipped) > 0 : \n",
    "    fixed_df_flipped = fix_flipped(df_flipped)\n",
    "    fixed_df = update_df_with_fixed_vals(fixed_df_flipped, df)\n",
    "if len(df_flipped_non_adjacent) > 0: \n",
    "    fixed_df_flipped_non_adjacent = fix_flipped(df_flipped_non_adjacent)\n",
    "    fixed_df = update_df_with_fixed_vals(fixed_df_flipped_non_adjacent, fixed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1546"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fixed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual fixes performed for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update(fixed_df, data):\n",
    "#     def update_vals(row, data=data):\n",
    "#         if row.index_original == data['index_original']:\n",
    "#             row.entity_number = data['entity_number']\n",
    "#         return row\n",
    "\n",
    "#     fixed_df = fixed_df.apply(update_vals, axis=1)\n",
    "#     return fixed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The objective of this study was to evaluate the effect of oral administration of ginseng stem - and - leaf saponins ( GSLS ) on the humoral immune responses of chickens to inactivated ND and AI vaccines .'"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_overlapping.iloc[4]['tokenized_sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "needed to label inactivated ND vaccines and inactivated AI vaccines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Below is for the df_overlapping of test data of MedLine for the extraction case\n",
    "\n",
    "# data = [{   'index_original' :  142, \n",
    "#             'entity_number': ([10], [12,13])\n",
    "#         }, \n",
    "#        {\n",
    "#            'index_original': 145, \n",
    "#            'entity_number': ([15], [17,18])\n",
    "#        },\n",
    "#        {\n",
    "#            'index_original': 151, \n",
    "#            'entity_number': ([12], [14,15])\n",
    "#        },\n",
    "#        {\n",
    "#            'index_original': 163, \n",
    "#            'entity_number': ([23], [29])\n",
    "#        },\n",
    "#        {\n",
    "#            'index_original': 170, \n",
    "#            'entity_number': ([32,33], [35,36])\n",
    "#        }]\n",
    "# fixed_df_overlapping = df_overlapping.copy()\n",
    "# for d in data:\n",
    "#     fixed_df_overlapping = update(fixed_df_overlapping, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Below is for the df_overlapping of test data of DrugBank for extraction case\n",
    "# data = [{   'index_original' :  974, \n",
    "#             'entity_number': ([2,3,4,5], [16])\n",
    "#         }, \n",
    "#        {\n",
    "#            'index_original': 1807, \n",
    "#            'entity_number': ([8,9], [18])\n",
    "#        },\n",
    "#        {\n",
    "#            'index_original': 3544, \n",
    "#            'entity_number': ([7,8], [30])\n",
    "#        },\n",
    "#        {\n",
    "#            'index_original': 3545, \n",
    "#            'entity_number': ([7,8], [51])\n",
    "#        },\n",
    "#        {\n",
    "#            'index_original': 3830, \n",
    "#            'entity_number': ([3], [5,6,7,8])\n",
    "#        },\n",
    "#        {\n",
    "#            'index_original': 3933, \n",
    "#            'entity_number': ([23,24], [26,27,28])\n",
    "#        },\n",
    "#        {\n",
    "#            'index_original': 4047, \n",
    "#            'entity_number': ([2,3,4,5], [14])\n",
    "#        }]\n",
    "\n",
    "# fixed_df_overlapping = df_overlapping.copy()\n",
    "# for d in data:\n",
    "#     fixed_df_overlapping = update(fixed_df_overlapping, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Examples of some of the more potent CYP 3A4 inhibitors include macrolide antibiotics ( e.g. , erythromycin , troleandomycin , clarithromycin ) , HIV protease or reverse transcriptase inhibitors ( e.g. , ritonavir , indinavir , nelfinavir , delavirdine ) or azole antifungals ( e.g. , ketoconazole , itraconazole , voriconazole ) .'"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_non_adjacent.iloc[8]['tokenized_sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Below is for the df_non_adjacent of test data of MedLine for the extraction case\n",
    "\n",
    "# data = [{   'index_original' :  140, \n",
    "#             'entity_number': ([0], [10])\n",
    "#         }, \n",
    "#        {\n",
    "#            'index_original': 143, \n",
    "#            'entity_number': ([0], [15])\n",
    "#        },\n",
    "#        {\n",
    "#            'index_original': 149, \n",
    "#            'entity_number': ([8], [12])\n",
    "#        },\n",
    "#        {\n",
    "#            'index_original': 161, \n",
    "#            'entity_number': ([9], [23])\n",
    "#        },\n",
    "#        {\n",
    "#            'index_original': 162, \n",
    "#            'entity_number': ([9], [29])\n",
    "#        },\n",
    "#        {\n",
    "#            'index_original': 166, \n",
    "#            'entity_number': ([14, 15, 16, 17, 18, 19, 20], [32,33])\n",
    "#        },\n",
    "#        {\n",
    "#            'index_original': 167, \n",
    "#            'entity_number': ([14, 15, 16, 17, 18, 19, 20], [35,36])\n",
    "#        },\n",
    "#        {\n",
    "#            'index_original': 168, \n",
    "#            'entity_number': ([22], [32,33])\n",
    "#        },\n",
    "#        {\n",
    "#            'index_original': 169, \n",
    "#            'entity_number': ([22], [35,36])\n",
    "#        }]\n",
    "# fixed_df_non_adjacent = df_non_adjacent.copy()\n",
    "# for d in data:\n",
    "#     fixed_df_non_adjacent = update(fixed_df_non_adjacent, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Below is for the df_non_adjacent of test data of DrugBank for extraction case\n",
    "# def update_df_non_adj_test_drugbank(row):\n",
    "#     entity_number = ast.literal_eval(row.entity_number)\n",
    "#     e1 = entity_number[0]\n",
    "#     e2 = entity_number[1]\n",
    "#     if e1 == [23,24,28]:\n",
    "#         e1 = [23,24]\n",
    "#     if e2 == [23,24,28]:\n",
    "#         e2 = [23,24]\n",
    "#     return e1, e2\n",
    "\n",
    "# fixed_df_non_adjacent = df_non_adjacent.copy()\n",
    "# fixed_df_non_adjacent['entity_number'] = df_non_adjacent.apply(update_df_non_adj_test_drugbank, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(fixed_df_overlapping) > 0: \n",
    "#     fixed_df = update_df_with_fixed_vals(fixed_df_overlapping, fixed_df)\n",
    "# if len(fixed_df_non_adjacent) > 0:\n",
    "#     fixed_df = update_df_with_fixed_vals(fixed_df_non_adjacent, fixed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we are going to do deletions (just for the train data) - for the test data, just fix the entity numbering and do not edit the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete df_overlapping and df_same_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_indexes_to_drop = df_overlapping['index_original'].tolist() + df_same_entity['index_original'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_indexes_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_df_with_dropped = fixed_df.drop(list_of_indexes_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1384"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fixed_df_with_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 669,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fixed_df_with_dropped) + len(list_of_indexes_to_drop) == len(fixed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write this fixed dataframe down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed_df_with_dropped.to_csv(res('Train/MedLine/train-medline_filtered.csv'), encoding='utf-8', index=False) #(for train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed_df.to_csv(res('Test/MedLine/test-medline_filtered.csv'), encoding='utf-8', index=False) # for test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the Train/Test data of MedLine and DrugBank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory2 = 'Test/MedLine/'\n",
    "file2 = 'test-medline_filtered'\n",
    "directory1 = 'Test/DrugBank/'\n",
    "file1 = 'test-drugbank_filtered'\n",
    "outfile = 'test_filtered.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/13613336/python-concatenate-text-files\n",
    "filenames = [res(directory1 + file1+'.txt'), res(directory2 + file2+'.txt')]\n",
    "with open(res(outfile), 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now handle the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_copy = pd.read_csv(res('Train/DrugBank/train-drugbank.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The bear ran home to the mountains'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1_pos = (1,1)\n",
    "e2_pos = (3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a tokenized and splitted sentence\n",
    "def sentence_replace(sentence, positions, string_update):\n",
    "    return sentence[:positions[0]] + [string_update] + sentence[positions[1]+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'bear', 'ran', 'DRUG', 'the', 'mountains']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_replace(['The', 'bear', 'ran', 'home', 'to', 'the', 'mountains'], (3,4), 'DRUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence is the sentence to update and entity positions is a list of entity positions\n",
    "def per_sentence_replacement_ddi(sentence, entity_positions):\n",
    "    # if entity position is updated, then all positions after it also have to be updated\n",
    "    sentence = sentence.split() # no need to do this for the code\n",
    "    \n",
    "    e0_pos = entity_positions[0]\n",
    "    sentence = sentence_replace(sentence, e0_pos, 'DRUG1')\n",
    "    new_e0_pos = (e0_pos[0], e0_pos[0])\n",
    "   \n",
    "    entity_positions[0] = new_e0_pos\n",
    "    diff = e0_pos[1] - e0_pos[0] # if the entity is 2 word, then move every other e_pos down by 1\n",
    "    if diff > 0:\n",
    "        for i in range(1, len(entity_positions)):\n",
    "            e_pos = entity_positions[i]\n",
    "            if e_pos[0] > e0_pos[1]:\n",
    "                entity_positions[i] = (entity_positions[i][0] - diff, entity_positions[i][1] - diff)\n",
    "     \n",
    "    e1_pos = entity_positions[1]\n",
    "    sentence = sentence_replace(sentence, e1_pos, 'DRUG2')\n",
    "    new_e1_pos = (e1_pos[0], e1_pos[0])\n",
    "    \n",
    "    entity_positions[1] = new_e1_pos\n",
    "    diff = e1_pos[1] - e1_pos[0]\n",
    "    if diff > 0 and len(entity_positions) > 2:\n",
    "        for i in range(2, len(entity_positions)):\n",
    "            e_pos = entity_positions[i]\n",
    "            if e_pos[0] > e1_pos[1]:\n",
    "                entity_positions[i] = (entity_positions[i][0] - diff, entity_positions[i][1] - diff)\n",
    "    # then should handle for the case when there are more than entity 1 and entity 2 i.e. drug0 (any other drug)\n",
    "    return sentence, entity_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['The', 'DRUG0', 'DRUG1', 'the', 'mountains'], [(1, 1), (2, 2)])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_entity_replacement('The bear ran home to the mountains', [(1,2), (3,4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/medg/misc/semeval_2010/medical-data/DDICorpus/pre-processed/extraction/Train/DrugBank/train-drugbank.txt'"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res('Train/DrugBank/train-drugbank.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite fixes, there is still data in DrugBank that is messed up, need to just manually delete those rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = open(res('Train/DrugBank/train-drugbank_filtered.txt'))\n",
    "train_data = utils.split_data_cut_sentence(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data = utils.replace_by_drug_ddi(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = []\n",
    "sentences, relations, e1_pos, e2_pos = new_train_data\n",
    "for i in range(len(sentences)):\n",
    "    sent = sentences[i]\n",
    "    pos1 = e1_pos[i]\n",
    "    pos2 = e2_pos[i]\n",
    "    sentence_len = len(sent)\n",
    "    if(pos1[0] >= sentence_len or pos2[0] >= sentence_len):\n",
    "        print(len(sent), i)\n",
    "        indexes.append(i)\n",
    "        print(sent, pos1, pos2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, relations, e1_pos, e2_pos = train_data\n",
    "for i in range(len(sentences)):\n",
    "    if i in indexes:\n",
    "        print(sentences[i], e1_pos[i], e2_pos[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these sentences in the csv files, it looks like they have that issue where they will say 'loop diuretics' is the entity but there are a bunch of words in between them in the sentence. This is a problem because for the predictive model, it expects entities to be next to each other, so while my algorithm is able to detect these entities correctly, unless I manually edit the sentences myself (which I can do), there is no way for the model to learn. For future reference, these exist in drugbank csv around 9371 if I want to fix it. Might actually make sense to edit these, and edit them for the test data as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "issues present in train_drugbank.csv and test_medline.csv and fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
