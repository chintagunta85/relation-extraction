{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More accurately concept blinding and removal of punctuation, normalizing digit, stop word removal, ner blinding (without stratification of types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Semeval 2010 dataset, this means replacing entity words by entity1 and entity2. For DDI dataset, this means replacing the two involved drug words and the other drug words. For the i2b2 dataset, this means replacing the words by their concept names. This blinding is entirely dataset dependent and by having the replacement information present in the metadata of the original csv file generated by the converters, the pre-processing module can be entirely separated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data pre-processing code has multiple assumptions. First of all, metadata that is generated should not contain any overlaps between words within the same entity itself (for example if e1 word_index is stored as [(3,3),(3,4)] that is not allowed, but [(3,3),(4,4)] is allowed. Further, if there is overlap between e1 and e2, they have to have an exact overlap and not a partial overlap. For example e1's word_index [(1,1),(2,2)] and e2's word_index [(1,1), (3,3)] is allowed but [(1,1), (2,2)] and [(1,2)] is not allowed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /afs/csail.mit.edu/u/g/geeticka/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "import os, pandas as pd, numpy as np\n",
    "import sys\n",
    "sys.path.append('../../../')\n",
    "from relation_extraction.data import utils\n",
    "import nltk\n",
    "from ast import literal_eval\n",
    "import itertools\n",
    "import spacy\n",
    "RESOURCE_PATH = \"/data/medg/misc/geeticka/relation_extraction/ddi\"\n",
    "indir = 'pre-processed/original/'\n",
    "outdir1 = 'pre-processed/entity_blinding/'\n",
    "outdir2 = 'pre-processed/punct_stop_digit/'\n",
    "outdir3 = 'pre-processed/punct_digit/'\n",
    "outdir4 = 'pre-processed/ner_blinding/'\n",
    "def res(path): return os.path.join(RESOURCE_PATH, path)\n",
    "from relation_extraction.data.converters.converter_ddi import write_dataframe, read_dataframe,\\\n",
    "check_equality_of_written_and_read_df, write_into_txt, combine\n",
    "# from relation_extraction.data.preprocess import replace_with_concept, replace_digit_punctuation_stop_word,\\\n",
    "# get_entity_positions_and_replacement_sentence\n",
    "from relation_extraction.data.preprocess import preprocess\n",
    "\n",
    "def makedir(outdir, res):\n",
    "    if not os.path.exists(res(outdir)):\n",
    "        os.makedirs(res(outdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en_core_sci_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good row to look at in drugbank data is 4123 df_train_drugbank.iloc[4123]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the different preprocessed versions into csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_dataframe_names = ['train_drugbank', 'train_medline', 'test_drugbank', 'test_medline']\n",
    "# makedir(outdir1, res)\n",
    "# makedir(outdir2, res)\n",
    "# makedir(outdir3, res)\n",
    "# makedir(outdir4, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for original_df_name in original_dataframe_names:\n",
    "#     type1 = preprocess(read_dataframe, res(indir + original_df_name + '_original.csv'))\n",
    "#     type2 = preprocess(read_dataframe, res(indir + original_df_name + '_original.csv'), 2)\n",
    "#     type3 = preprocess(read_dataframe, res(indir + original_df_name + '_original.csv'), 3)\n",
    "#     type4 = preprocess(read_dataframe, res(indir + original_df_name + '_original.csv'), nlp, 4)\n",
    "#     write_dataframe(type1, res(outdir1 + original_df_name + '_entity_blinding.csv'))\n",
    "#     write_dataframe(type2, res(outdir2 + original_df_name + '_punct_stop_digit.csv'))\n",
    "#     write_dataframe(type3, res(outdir3 + original_df_name + '_punct_digit.csv'))\n",
    "#     write_dataframe(type4, res(outdir4 + original_df_name + '_ner_blinding.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write into text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique relations: \t ['effect' 'none' 'advise' 'mechanism' 'int']\n",
      "Unique relations: \t ['none' 'mechanism' 'effect' 'advise' 'int']\n",
      "Unique relations: \t ['int' 'effect' 'none' 'mechanism' 'advise']\n",
      "Unique relations: \t ['none' 'effect' 'mechanism' 'advise' 'int']\n"
     ]
    }
   ],
   "source": [
    "# for original_df_name in original_dataframe_names:\n",
    "#     type1 = read_dataframe(res(outdir1 + original_df_name + '_entity_blinding.csv'))\n",
    "#     type2 = read_dataframe(res(outdir2 + original_df_name + '_punct_stop_digit.csv'))\n",
    "#     type3 = read_dataframe(res(outdir3 + original_df_name + '_punct_digit.csv'))\n",
    "#     type4 = read_dataframe(res(outdir4 + original_df_name + '_ner_blinding.csv'))\n",
    "#     write_into_txt(type1, res(outdir1 + original_df_name + '_entity_blinding.txt'))\n",
    "#     write_into_txt(type2, res(outdir2 + original_df_name + '_punct_stop_digit.txt'))\n",
    "#     write_into_txt(type3, res(outdir3 + original_df_name + '_punct_digit.txt'))\n",
    "#     write_into_txt(type4, res(outdir4 + original_df_name + '_ner_blinding.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the train and test data of drugbank and medline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in ['train', 'test']:\n",
    "#     combine(res, outdir1, i + '_drugbank_entity_blinding', i + '_medline_entity_blinding', i + '_entity_blinding.txt')\n",
    "#     combine(res, outdir2, i + '_drugbank_punct_stop_digit', i + '_medline_punct_stop_digit', i + '_punct_stop_digit.txt')\n",
    "#     combine(res, outdir3, i + '_drugbank_punct_digit', i + '_medline_punct_digit', i + '_punct_digit.txt')\n",
    "#     combine(res, outdir4, i + '_drugbank_ner_blinding', i + '_medline_ner_blinding', i + '_ner_blinding.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that the lengths of the files created is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_file_length(res, filename):\n",
    "    return len(open(res(filename)).readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21233\n",
      "21233\n",
      "21233\n",
      "21233\n",
      "21233\n"
     ]
    }
   ],
   "source": [
    "print(output_file_length(res, indir + 'train_original.txt'))\n",
    "print(output_file_length(res, outdir1 + 'train_entity_blinding.txt'))\n",
    "print(output_file_length(res, outdir2 + 'train_punct_stop_digit.txt'))\n",
    "print(output_file_length(res, outdir3 + 'train_punct_digit.txt'))\n",
    "print(output_file_length(res, outdir4 + 'train_ner_blinding.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4696\n",
      "4696\n",
      "4696\n",
      "4696\n",
      "4696\n"
     ]
    }
   ],
   "source": [
    "print(output_file_length(res, indir + 'test_original.txt'))\n",
    "print(output_file_length(res, outdir1 + 'test_entity_blinding.txt'))\n",
    "print(output_file_length(res, outdir2 + 'test_punct_stop_digit.txt'))\n",
    "print(output_file_length(res, outdir3 + 'test_punct_digit.txt'))\n",
    "print(output_file_length(res, outdir4 + 'test_ner_blinding.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
