{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please note: Using spacy version 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /afs/csail.mit.edu/u/g/geeticka/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "from xml.etree.ElementTree import tostring\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk import wordnet as wn\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume in this case that all sentences have been tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bear det The 1 0\n",
      "ran nsubj bear 2 1\n",
      "ran ROOT ran 2 2\n",
      "ran advmod home 2 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['The', 'bear', 'ran', 'home'],\n",
       " ['the', 'bear', 'run', 'home'],\n",
       " ['DET', 'NOUN', 'VERB', 'ADV'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up spaCy\n",
    "# from spacy.en import English\n",
    "# from spacy.symbols import ORTH, LEMMA, POS\n",
    "#parser = English()\n",
    "parser = spacy.load('en')\n",
    "def word_tokenize_features(parsedData):\n",
    "    words = []\n",
    "    lemmas = []\n",
    "    pos = []\n",
    "    for span in parsedData.sents:\n",
    "        sent = [parsedData[i] for i in range(span.start, span.end)]\n",
    "        for token in sent:\n",
    "            words.append(token.orth_)\n",
    "            lemmas.append(token.lemma_)\n",
    "            pos.append(token.pos_)\n",
    "            print(token.head.text, token.dep_, token.text, token.head.i, token.i)\n",
    "    return words, lemmas, pos\n",
    "  \n",
    "def stringify_tokenized(tokenizedSentence):\n",
    "    return \" \".join(tokenizedSentence)\n",
    "\n",
    "tokenized = ['The', 'bear', 'ran', 'home']\n",
    "sentence = stringify_tokenized(tokenized)\n",
    "parsedData = parser(sentence)\n",
    "word_tokenize_features(parsedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different ways of getting the hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input must be a tokenized sentence in the form ['There', 'is', 'a', 'dog', '.']\n",
    "def get_hypernyms(tokenizedSentence):\n",
    "    hypernyms = []\n",
    "    for word in tokenizedSentence:\n",
    "        hypernym_perword = []\n",
    "        if len(wn.wordnet.synsets(word)) == 0:\n",
    "            hypernyms.append(\"\")\n",
    "            continue\n",
    "        foundhypernym = 0\n",
    "        for synset in wn.wordnet.synsets(word):\n",
    "            if len(synset.hypernyms()) > 0:\n",
    "                hypernym = synset.hypernyms()[0]\n",
    "                foundhypernym += 1\n",
    "                hypernyms.append(hypernym.name())\n",
    "                break\n",
    "        if foundhypernym == 0:\n",
    "            hypernyms.append(\"\")\n",
    "    return hypernyms\n",
    "\n",
    "# given a part of speech as a string, return a wordnet part of speech\n",
    "def toWordnetPOS(POS):\n",
    "    if(POS == 'VERB'):\n",
    "        return wn.wordnet.VERB\n",
    "    elif(POS == 'NOUN'):\n",
    "        return wn.wordnet.NOUN\n",
    "    elif(POS == 'ADJ'):\n",
    "        return wn.wordnet.ADJ\n",
    "    elif(POS == 'ADV'):\n",
    "        return wn.wordnet.ADV\n",
    "    return None\n",
    "    \n",
    "# the following hypernym method takes in the words with their respective part of speech to give a hypernym that fits accordingly\n",
    "def get_hypernyms_usingPOS(tokenized):\n",
    "    hypernyms = []\n",
    "    sentence = stringify_tokenized(tokenized)\n",
    "    parsedData = parser(sentence)\n",
    "    _, _, POS = word_tokenize_features(parsedData)\n",
    "    for i in range(len(tokenized)):\n",
    "        word = tokenized[i]\n",
    "        print(POS[i])\n",
    "        pos = toWordnetPOS(POS[i])\n",
    "        hypernym_perword = []\n",
    "        if len(wn.wordnet.synsets(word, pos=pos)) == 0:\n",
    "            hypernyms.append(\"\")\n",
    "            continue\n",
    "        foundhypernym = 0\n",
    "        for synset in wn.wordnet.synsets(word, pos=pos):\n",
    "            if len(synset.hypernyms()) > 0:\n",
    "                hypernym = synset.hypernyms()[0]\n",
    "                foundhypernym += 1\n",
    "                hypernyms.append(hypernym.name())\n",
    "                break\n",
    "        if foundhypernym == 0:\n",
    "            hypernyms.append(\"\")\n",
    "    return hypernyms\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'carnivore.n.01', 'travel_rapidly.v.01', 'residence.n.01']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_hypernyms(['The', 'bear', 'ran', 'home'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bear det The 1 0\n",
      "ran nsubj bear 2 1\n",
      "ran ROOT ran 2 2\n",
      "ran advmod home 2 3\n",
      "DET\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['', 'carnivore.n.01', 'travel_rapidly.v.01', '']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wn.wordnet.synsets('bear', pos=wn.wordnet.VERB)\n",
    "# NOUN, ADJ and ADV are the other possibilities\n",
    "\n",
    "get_hypernyms_usingPOS(['The', 'bear', 'ran', 'home'])\n",
    "# this reveals that I should keep it simple for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is expl There 1 0\n",
      "is ROOT is 1 1\n",
      "dog det a 3 2\n",
      "is attr dog 1 3\n",
      "dog prep across 3 4\n",
      "street det the 6 5\n",
      "across pobj street 4 6\n",
      "is punct . 1 7\n",
      "['There', 'is', 'a', 'dog', 'across', 'the', 'street', '.'] ['there', 'be', 'a', 'dog', 'across', 'the', 'street', '.'] ['ADV', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "onesentence = stringify_tokenized(['There', 'is', 'a', 'dog', 'across', 'the', 'street', '.'])\n",
    "parsedData = parser(onesentence)\n",
    "words, lemmas, pos = word_tokenize_features(parsedData)\n",
    "print(words, lemmas, pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the n, 01, _ from the strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = \"travel_rapidly.v.01\"\n",
    "word1 = \" \".join(re.findall(\"[a-zA-Z]+\", st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'travel rapidly v'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.wordnet.VERB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.wordnet.ADJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.wordnet.ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_character = word1[len(word1)-1:]\n",
    "if(last_character == 'v' or last_character == 'n' or last_character == 'a' or last_character == 'r'):\n",
    "    word1 = word1[:len(word1)-1]\n",
    "    word1 = word1.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'travel rapidly'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'travel'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1.split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function generates the hypernyms of a tokenized sentence and splits out only the words\n",
    "# it has a flag to indicate whether to give only the first word or all of them\n",
    "def get_hypernyms_onlywords(tokenizedSentence, onlyFirstWord=False):\n",
    "    hypernyms = get_hypernyms(tokenizedSentence)\n",
    "    hypernyms_onlywords = []\n",
    "    for hypernym in hypernyms:\n",
    "        if hypernym == '':\n",
    "            hypernyms_onlywords.append(hypernym)\n",
    "            continue\n",
    "        word = \" \".join(re.findall(\"[a-zA-Z]+\", hypernym))\n",
    "        last_character = word[len(word)-1:]\n",
    "        if(last_character == 'v' or last_character == 'n' or last_character == 'a' or last_character == 'r'):\n",
    "            word = word[:len(word)-1]\n",
    "            word = word.strip()\n",
    "        if onlyFirstWord == True:\n",
    "            word = word.split()[0]\n",
    "        hypernyms_onlywords.append(word)\n",
    "    return hypernyms_onlywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'carnivore', 'travel rapidly', 'residence']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_hypernyms_onlywords(['The', 'bear', 'ran', 'home'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'carnivore', 'travel', 'residence']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_hypernyms_onlywords(['The', 'bear', 'ran', 'home'], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Testing Di's split_data_cut_sentence method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 and 11 are opposites\n",
    "# 1 and 16 are opposites\n",
    "# 2 and 8 are opposites\n",
    "# 3 and 10 are opposites\n",
    "# 4 and 17 are opposites\n",
    "# 5 and 15 are opposites\n",
    "# 6 and 12 are opposites\n",
    "# 7 and 13 are opposites\n",
    "# 9 and 14 are opposites\n",
    "# 18 does not have an opposite\n",
    "# given a relation as a number, return the number of the opposite relation\n",
    "def give_reverse_relation(relation):\n",
    "    reverse_dict = {0: 11, 11:0, 1:16, 16:1, 2:8, 8:2, 3:10, 10:3, 4:17, \n",
    "                    17:4, 5:15, 15:5, 6:12, 12:6, 7:13, 13:7, 9:14, 14:9, 18:18}\n",
    "    return reverse_dict[relation]\n",
    "give_reverse_relation(18)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['1 1 2 3 4 Hi my name is beaver', '2 1 2 4 5 My name is tomorrow blah blah']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted = utils.split_data_cut_sentence(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(data):\n",
    "    sentences = data[0]\n",
    "    relations = data[1]\n",
    "    e1_pos = data[2]\n",
    "    e2_pos = data[3]\n",
    "    augmented_sentences = []\n",
    "    augmented_relations = []\n",
    "    augmented_e1_pos = []\n",
    "    augmented_e2_pos = []\n",
    "    augmented_relations = []\n",
    "    for idx, (sent, pos1, pos2, rel) in enumerate(zip(sentences, e1_pos, e2_pos, relations)):\n",
    "        reversed_sent = list(reversed(sent))\n",
    "        reversed_rel = give_reverse_relation(rel)\n",
    "        reversed_pos1_first = len(sent) - (pos2[1] + 1)\n",
    "        reversed_pos1_second = len(sent) - (pos2[0] + 1)\n",
    "        reversed_pos2_first = len(sent) - (pos1[1] + 1)\n",
    "        reversed_pos2_second = len(sent) - (pos1[0] + 1)\n",
    "        reversed_pos1 = (reversed_pos1_first, reversed_pos1_second)\n",
    "        reversed_pos2 = (reversed_pos2_first, reversed_pos2_second)\n",
    "        augmented_sentences.append(reversed_sent)\n",
    "        augmented_relations.append(reversed_rel)\n",
    "        augmented_e1_pos.append(reversed_pos1)\n",
    "        augmented_e2_pos.append(reversed_pos2)\n",
    "    sentences = sentences + augmented_sentences\n",
    "    relations = relations + augmented_relations\n",
    "    e1_pos = e1_pos + augmented_e1_pos\n",
    "    e2_pos = e2_pos + augmented_e2_pos\n",
    "    \n",
    "    return sentences, relations, e1_pos, e2_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['hi', 'my', 'name', 'is', 'beaver'],\n",
       "  ['my', 'name', 'is', 'tomorrow', 'blah', 'blah'],\n",
       "  ['beaver', 'is', 'name', 'my', 'hi'],\n",
       "  ['blah', 'blah', 'tomorrow', 'is', 'name', 'my']],\n",
       " [1, 2, 16, 8],\n",
       " [(1, 2), (1, 2), (0, 1), (0, 1)],\n",
       " [(3, 4), (4, 5), (2, 3), (3, 4)])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augment_data(splitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Get the shortest dependency path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, how to get the dependency parse itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cars amod Autonomous 1 0\n",
      "shift nsubj cars 2 1\n",
      "shift ROOT shift 2 2\n",
      "liability compound insurance 4 3\n",
      "shift dobj liability 2 4\n",
      "shift prep toward 2 5\n",
      "toward pobj manufacturers 5 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Autonomous',\n",
       "  'cars',\n",
       "  'shift',\n",
       "  'insurance',\n",
       "  'liability',\n",
       "  'toward',\n",
       "  'manufacturers'],\n",
       " ['autonomous',\n",
       "  'car',\n",
       "  'shift',\n",
       "  'insurance',\n",
       "  'liability',\n",
       "  'toward',\n",
       "  'manufacturer'],\n",
       " ['ADJ', 'NOUN', 'VERB', 'NOUN', 'NOUN', 'ADP', 'NOUN'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenized = ['The', 'bear', 'ran', 'home']\n",
    "tokenized = [\"Autonomous\", \"cars\", \"shift\", \"insurance\", \"liability\", \"toward\", \"manufacturers\"]\n",
    "sentence = stringify_tokenized(tokenized)\n",
    "parsedData = parser(sentence)\n",
    "word_tokenize_features(parsedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars cars nsubj shift\n",
      "insurance liability liability dobj shift\n",
      "manufacturers manufacturers pobj toward\n"
     ]
    }
   ],
   "source": [
    "for chunk in parsedData.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "          chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1_pos = (1,1)\n",
    "e2_pos = (3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/32835291/how-to-find-the-shortest-dependency-path-between-two-words-in-python\n",
    "# above is very useful in knowing how to calculate the shortest dependency path using the graphs package as well as spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "['cars_1', 'shift_2', 'liability_4', 'insurance_3']\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import spacy\n",
    "# nlp = spacy.load('en')\n",
    "\n",
    "# # https://spacy.io/docs/usage/processing-text\n",
    "# document = nlp(u'Robots in popular culture are there to remind us of the awesomeness of unbound human agency.', parse=True)\n",
    "\n",
    "# print('document: {0}'.format(document))\n",
    "\n",
    "# Load spacy's dependency tree into a networkx graph\n",
    "edges = []\n",
    "for token in parsedData:\n",
    "    # FYI https://spacy.io/docs/api/token\n",
    "    for child in token.children:\n",
    "        edges.append(('{0}_{1}'.format(token.lower_,token.i),\n",
    "                      '{0}_{1}'.format(child.lower_,child.i), \n",
    "                     {'name': token.dep_}))\n",
    "#         edges.append(({'word':token.lower_, 'pos':str(token.i)}, {'word':child.lower_, 'pos':str(child.i)}))\n",
    "\n",
    "graph = nx.Graph(edges)\n",
    "entity1 = tokenized[e1_pos[0]]+ \"_\" + str(e1_pos[0]) #if they are of size 1\n",
    "entity2 = tokenized[e2_pos[0]]+ \"_\" + str(e2_pos[0])\n",
    "# https://networkx.github.io/documentation/networkx-1.10/reference/algorithms.shortest_paths.html\n",
    "print(nx.shortest_path_length(graph, source=entity1, target=entity2))\n",
    "print(nx.shortest_path(graph, source=entity1, target=entity2))\n",
    "# print(nx.shortest_path(graph, source='robots-0', target='agency-15'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can get away with an embedding of size 10 or 5 because the number of possible dependencies are on the order of 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy import displacy\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "# displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attributes = nx.get_edge_attributes(graph, 'name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('cars_1', 'autonomous_0'): 'nsubj',\n",
       " ('cars_1', 'shift_2'): 'ROOT',\n",
       " ('shift_2', 'liability_4'): 'ROOT',\n",
       " ('shift_2', 'toward_5'): 'ROOT',\n",
       " ('liability_4', 'insurance_3'): 'dobj',\n",
       " ('toward_5', 'manufacturers_6'): 'prep'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nsubj'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_attributes[('cars_1', 'autonomous_0')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = nx.shortest_path(graph, source=entity1, target=entity2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cars_1', 'shift_2', 'liability_4', 'insurance_3']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_with_names = []\n",
    "for i in range(0, len(path)-1):\n",
    "    path_with_names.append(path[i])\n",
    "    edge = edge_attributes[(path[i], path[i+1])]\n",
    "    path_with_names.append(edge)\n",
    "path_with_names.append(path[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cars_1', 'ROOT', 'shift_2', 'ROOT', 'liability_4', 'dobj', 'insurance_3']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_with_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import spacy\n",
    "\n",
    "def get_path_length(entity1, entity2, graph):\n",
    "    if nx.has_path(graph, entity1, entity2):\n",
    "        length = nx.shortest_path_length(graph, source=entity1, target=entity2)\n",
    "        return length\n",
    "    else: \n",
    "        return None\n",
    "\n",
    "def get_path_with_edge_name(graph, path):\n",
    "    edge_attributes = nx.get_edge_attributes(graph, 'name')\n",
    "    path_with_edge_names = []\n",
    "    for i in range(0, len(path)-1):\n",
    "        path_with_edge_names.append(path[i])\n",
    "        w1 = path[i]\n",
    "        w2 = path[i+1]\n",
    "        #edge_attributes give a great idea of directionality\n",
    "        # 0 means direction is from left to right (where left is the head)\n",
    "        # 1 means direction is from right to left (where right is the head)\n",
    "        edge = edge_attributes[(w1,w2) if (w1,w2) in edge_attributes else (w2, w1)] \\\n",
    "        + \"_\" + str(0) if (w1,w2) in edge_attributes else str(1)\n",
    "        path_with_edge_names.append(edge)\n",
    "    path_with_edge_names.append(path[-1])\n",
    "    return path_with_edge_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import spacy\n",
    "def get_shortest_dependency_path(tokenizedSentence, e1_pos, e2_pos):\n",
    "    parser = spacy.load('en')\n",
    "    sentence = stringify_tokenized(tokenizedSentence)\n",
    "    parsedData = parser(sentence)\n",
    "    edges = []\n",
    "    print(\"Tokenized sentence, sentence\", tokenizedSentence, sentence)\n",
    "    # it is possible that spacy is giving extra tokens to the sentence than are needed\n",
    "    for token in parsedData:\n",
    "        # FYI https://spacy.io/docs/api/token\n",
    "        for child in token.children:\n",
    "            edges.append(('{0}_{1}'.format(token.lower_,token.i), #head\n",
    "                          '{0}_{1}'.format(child.lower_,child.i), #child\n",
    "                         {'name': token.dep_})) #name of the dependency\n",
    "\n",
    "    graph = nx.Graph(edges)\n",
    "    #TODO: handle the case with multiple words in the entity\n",
    "    # basically do below, but in a loop\n",
    "    # only do path_with_edge names for the shortest dependency path. \n",
    "    # length can be gotten using nx.shortest_path_length(graph, source=entity1, target=entity2)\n",
    "    entity1 = [tokenizedSentence[e1_pos[i]]+ \"_\" + str(e1_pos[i]) for i in range(0, len(e1_pos))]\n",
    "#     entity1 = tokenizedSentence[e1_pos[0]]+ \"_\" + str(e1_pos[0]) #if they are of size 1\n",
    "    entity2 = [tokenizedSentence[e2_pos[i]]+ \"_\" + str(e2_pos[i]) for i in range(0, len(e2_pos))]\n",
    "#     entity2 = tokenizedSentence[e2_pos[0]]+ \"_\" + str(e2_pos[0])\n",
    "    path_length = []\n",
    "    for e1 in entity1:\n",
    "        for e2 in entity2:\n",
    "            length = get_path_length(e1, e2, graph)\n",
    "            if length is not None:\n",
    "                path_length.append({'len': length, 'e1': e1, 'e2': e2})\n",
    "    print(\"Possible path lengths are \", path_length)\n",
    "    # TODO: make above more efficient; what is the maximum possible path length in dependency path in a sentence\n",
    "    if not path_length: # means list is empty which means that path didnt exist at all\n",
    "        return None, None\n",
    "    minItem = min(path_length, key=lambda x: x['len'])\n",
    "    path = nx.shortest_path(graph, source=minItem['e1'], target=minItem['e2'])\n",
    "    path_with_edge_names = get_path_with_edge_name(graph, path)\n",
    "#     if nx.has_path(graph,entity1,entity2):\n",
    "#         path = nx.shortest_path(graph, source=entity1, target=entity2)\n",
    "#     else: \n",
    "#         return None, None\n",
    "    # https://networkx.github.io/documentation/networkx-1.10/reference/algorithms.shortest_paths.html\n",
    "#     edge_attributes = nx.get_edge_attributes(graph, 'name')\n",
    "#     path_with_edge_names = []\n",
    "#     for i in range(0, len(path)-1):\n",
    "#         path_with_edge_names.append(path[i])\n",
    "#         w1 = path[i]\n",
    "#         w2=path[i+1]\n",
    "#         edge = edge_attributes[(w1,w2) if (w1,w2) in edge_attributes else (w2, w1)]\n",
    "#         path_with_edge_names.append(edge)\n",
    "#     path_with_edge_names.append(path[-1])\n",
    "    return path, path_with_edge_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence, sentence ['Autonomous', 'cars', 'shift', 'insurance', 'liability', 'toward', 'manufacturers'] Autonomous cars shift insurance liability toward manufacturers\n",
      "Possible path lengths are  [{'len': 3, 'e1': 'cars_1', 'e2': 'insurance_3'}, {'len': 3, 'e1': 'cars_1', 'e2': 'insurance_3'}, {'len': 3, 'e1': 'cars_1', 'e2': 'insurance_3'}, {'len': 3, 'e1': 'cars_1', 'e2': 'insurance_3'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['cars_1', 'shift_2', 'liability_4', 'insurance_3'],\n",
       " ['cars_1',\n",
       "  'ROOT_0',\n",
       "  'shift_2',\n",
       "  'ROOT_0',\n",
       "  'liability_4',\n",
       "  'dobj_0',\n",
       "  'insurance_3'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_shortest_dependency_path([\"Autonomous\", \"cars\", \"shift\", \"insurance\", \"liability\", \"toward\", \"manufacturers\"], (1,1), (3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_word(string):\n",
    "    return \" \".join(re.findall(\"[a-zA-Z]+\", string))\n",
    "\n",
    "def get_only_number(string):\n",
    "    return \" \".join(re.findall(\"[0-9]+\", string))\n",
    "\n",
    "def get_indiv_pos(position, words_num_dict, words_new_num_dict):\n",
    "    if str(position) not in words_num_dict:\n",
    "        return None\n",
    "    word = words_num_dict[str(position)]\n",
    "    return words_new_num_dict[word]\n",
    "\n",
    "def get_new_entity_position(words, e1_pos, e2_pos, numbers):\n",
    "    d1 = {numbers[i]: word for i, word in enumerate(words)} # words_num_dict\n",
    "    d2 = {word: i for i, word in enumerate(words)} # words_new_num_dict\n",
    "#     print(\"e1_pos, e2_pos\", e1_pos, e2_pos)\n",
    "    # TODO: new_position may include just one word inside of the entities\n",
    "    # which means that some word between the start and end might be in the path\n",
    "    #i.e. we need to check for every entity word if None is returned by get_indiv_pos\n",
    "    # because exactly one word's val returned will not be None\n",
    "    for i in range(e1_pos[0], e1_pos[1] + 1):\n",
    "        new_pos = get_indiv_pos(i, d1, d2)\n",
    "        if new_pos is not None:\n",
    "            new_e1_pos = (new_pos, new_pos)\n",
    "    for i in range(e2_pos[0], e2_pos[1] + 1):\n",
    "        new_pos = get_indiv_pos(i, d1, d2)\n",
    "        if new_pos is not None:\n",
    "            new_e2_pos = (new_pos, new_pos)\n",
    "#     new_e1_pos = (get_indiv_pos(e1_pos[0], d1, d2), get_indiv_pos(e1_pos[1], d1, d2))\n",
    "#     new_e2_pos = (get_indiv_pos(e2_pos[0], d1, d2), get_indiv_pos(e2_pos[1], d1, d2))\n",
    "    return new_e1_pos, new_e2_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the augment method to incorporate the dependency path information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(data, simple=True):\n",
    "    sentences = data[0]\n",
    "    relations = data[1]\n",
    "    e1_pos = data[2]\n",
    "    e2_pos = data[3]\n",
    "    augmented_sentences = []\n",
    "    augmented_relations = []\n",
    "    augmented_e1_pos = []\n",
    "    augmented_e2_pos = []\n",
    "    augmented_relations = []\n",
    "    for idx, (sent, pos1, pos2, rel) in enumerate(zip(sentences, e1_pos, e2_pos, relations)):\n",
    "        if simple is True: # in this case everything is reversed\n",
    "            augmented_sent = list(reversed(sent))\n",
    "            augmented_rel = give_reverse_relation(rel)\n",
    "            augmented_pos1_first = len(sent) - (pos2[1] + 1)\n",
    "            augmented_pos1_second = len(sent) - (pos2[0] + 1)\n",
    "            augmented_pos2_first = len(sent) - (pos1[1] + 1)\n",
    "            augmented_pos2_second = len(sent) - (pos1[0] + 1)\n",
    "            augmented_pos1 = (reversed_pos1_first, reversed_pos1_second)\n",
    "            augmented_pos2 = (reversed_pos2_first, reversed_pos2_second)\n",
    "        else: # in this case only the shortest dependency path is considered\n",
    "            print(\"Sentence\", sent)\n",
    "            path = get_shortest_dependency_path(sent, pos1, pos2)[0]\n",
    "            if path is None:\n",
    "                continue\n",
    "            print(\"Path\", path)\n",
    "            oldnums = [get_only_number(word) for word in path]\n",
    "            print(\"Old indexing\", oldnums)\n",
    "            augmented_sent = [get_only_word(word) for word in path]\n",
    "            augmented_pos1, augmented_pos2 = get_new_entity_position(augmented_sent, pos1, pos2, oldnums)\n",
    "            augmented_rel = rel\n",
    "        augmented_sentences.append(augmented_sent)\n",
    "        augmented_relations.append(augmented_rel)\n",
    "        augmented_e1_pos.append(augmented_pos1)\n",
    "        augmented_e2_pos.append(augmented_pos2)\n",
    "    sentences = sentences + augmented_sentences\n",
    "    relations = relations + augmented_relations\n",
    "    e1_pos = e1_pos + augmented_e1_pos\n",
    "    e2_pos = e2_pos + augmented_e2_pos\n",
    "    \n",
    "    return sentences, relations, e1_pos, e2_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['hi', 'my', 'name', 'is', 'beaver'],\n",
       "  ['my', 'name', 'is', 'tomorrow', 'blah', 'blah']],\n",
       " [1, 2],\n",
       " [(1, 2), (1, 2)],\n",
       " [(3, 4), (4, 5)])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmented data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence ['hi', 'my', 'name', 'is', 'beaver']\n",
      "Tokenized sentence, sentence ['hi', 'my', 'name', 'is', 'beaver'] hi my name is beaver\n",
      "Possible path lengths are  [{'len': 2, 'e1': 'my_1', 'e2': 'is_3'}, {'len': 3, 'e1': 'my_1', 'e2': 'beaver_4'}, {'len': 1, 'e1': 'name_2', 'e2': 'is_3'}, {'len': 2, 'e1': 'name_2', 'e2': 'beaver_4'}]\n",
      "Path ['name_2', 'is_3']\n",
      "Old indexing ['2', '3']\n",
      "Sentence ['my', 'name', 'is', 'tomorrow', 'blah', 'blah']\n",
      "Tokenized sentence, sentence ['my', 'name', 'is', 'tomorrow', 'blah', 'blah'] my name is tomorrow blah blah\n",
      "Possible path lengths are  []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([['hi', 'my', 'name', 'is', 'beaver'],\n",
       "  ['my', 'name', 'is', 'tomorrow', 'blah', 'blah'],\n",
       "  ['name', 'is']],\n",
       " [1, 2, 1],\n",
       " [(1, 2), (1, 2), (0, 0)],\n",
       " [(3, 4), (4, 5), (1, 1)])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augment_data(splitted, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to disable the tokenizer in spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms. : 0 : compound\n",
      "sendler : 1 : nsubj\n",
      "made : 2 : ROOT\n",
      "lists : 3 : dobj\n",
      "of : 4 : prep\n",
      "these : 5 : det\n",
      "children : 6 : pobj\n",
      "and : 7 : cc\n",
      "placed : 8 : conj\n",
      "the : 9 : det\n",
      "lists : 10 : dobj\n",
      "in : 11 : prep\n",
      "a : 12 : det\n",
      "jar : 13 : pobj\n",
      "that : 14 : dobj\n",
      "she : 15 : nsubj\n",
      "buried : 16 : relcl\n",
      "in : 17 : prep\n",
      "a : 18 : det\n",
      "garden : 19 : pobj\n",
      ". : 20 : punct\n"
     ]
    }
   ],
   "source": [
    "sentence = ['ms.', 'sendler', 'made', 'lists', 'of', 'these', 'children', 'and', 'placed', 'the', \\\n",
    "            'lists', 'in', 'a', 'jar', 'that', 'she', 'buried', 'in', 'a', 'garden', '.']\n",
    "e1_pos= (10, 10)\n",
    "e2_pos= (13,13)\n",
    "parser = spacy.load('en')\n",
    "from spacy.tokens import Doc\n",
    "doc = Doc(parser.vocab, words=sentence)\n",
    "parser.tagger(doc)\n",
    "parser.parser(doc)\n",
    "for token in doc:\n",
    "    print(token.lower_, \":\", token.i, \":\", token.dep_)\n",
    "    \n",
    "# for the pipeline, look at this https://spacy.io/usage/processing-pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting position embeddings from path without labels to path with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
