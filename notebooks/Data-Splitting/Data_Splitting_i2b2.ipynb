{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import os, random, pandas as pd, numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import relation_extraction.data.utils as utils\n",
    "import itertools\n",
    "# SEED = 1\n",
    "# np.random.seed(SEED)\n",
    "# random.seed(SEED)\n",
    "preprocessing_type = 'original'\n",
    "RESOURCE_PATH = \"/data/medg/misc/geeticka/relation_extraction/i2b2/pre-processed/\" + preprocessing_type\n",
    "def res(path): return os.path.join(RESOURCE_PATH, path)\n",
    "if not os.path.exists(res('pickled-files/')):\n",
    "    os.mkdir(res('pickled-files/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = open(res('train_{}.txt').format(preprocessing_type))\n",
    "splitted_data_border1 = utils.split_data_cut_sentence(train_data, border_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = open(res('train_{}.txt').format(preprocessing_type))\n",
    "splitted_data_border20 = utils.split_data_cut_sentence(train_data, border_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = open(res('train_{}.txt').format(preprocessing_type))\n",
    "splitted_data_border50 = utils.split_data_cut_sentence(train_data, border_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = open(res('train_{}.txt').format(preprocessing_type))\n",
    "splitted_data_border_minus1 = utils.split_data_cut_sentence(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(splitted_data):\n",
    "    # now need to figure out how to create the csv file\n",
    "    sen = splitted_data[0]\n",
    "    rel = splitted_data[1]\n",
    "    e1 = splitted_data[2]\n",
    "    e2 = splitted_data[3]\n",
    "    data = pd.DataFrame({'sentences': sen, 'relations': rel, 'e1_pos': e1, 'e2_pos': e2}, \n",
    "                        columns=['sentences', 'relations', 'e1_pos', 'e2_pos'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_border1 = get_dataframe(splitted_data_border1)\n",
    "data_border20 = get_dataframe(splitted_data_border20)\n",
    "data_border50 = get_dataframe(splitted_data_border50)\n",
    "data_borderminus1 = get_dataframe(splitted_data_border_minus1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of train data is 2117 and size of test data is 3992 sentences. It is difficult to achieve a similar split in this case because train data is half of the test data. Generally, we train on a larger amount of data than we use for development. So I am choosing to do 5 fold cross validation like in the DDI corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_data(data, filename1, filename2, SEED, preprocessing_type):\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    print(\"seed is\", SEED)\n",
    "    N = len(data)\n",
    "    K = 5\n",
    "\n",
    "    splitter = StratifiedKFold(n_splits=K, shuffle=True, random_state=SEED)\n",
    "    fold_indices = [indices for _, indices in splitter.split(\n",
    "        data[['sentences', 'e1_pos', 'e2_pos']].values,\n",
    "        data['relations'].values\n",
    "    )]\n",
    "\n",
    "    assert len(fold_indices) == K, \"Incorrect number of folds.\"\n",
    "    assert len(np.concatenate(fold_indices)) == N, \"Folds not comprehensive.\"\n",
    "\n",
    "    splits = []\n",
    "    for fold in range(K):\n",
    "        test_fold  = fold_indices[fold]\n",
    "        dev_fold   = fold_indices[(fold + 1) % K]\n",
    "\n",
    "        non_train_start = fold\n",
    "        non_train_end   = (fold + 2) % K\n",
    "\n",
    "        if non_train_start < non_train_end:\n",
    "            train_fold = np.concatenate(fold_indices[non_train_end:] + fold_indices[:non_train_start])\n",
    "        else: train_fold = np.concatenate(fold_indices[non_train_end:non_train_start])\n",
    "        joined = np.concatenate((train_fold, dev_fold, test_fold))\n",
    "\n",
    "        assert len(joined) == N, (\n",
    "            \"Split not comprehensive for fold {fold}:\\n\"\n",
    "            \"  len(train_fold): {train}\\n\"\n",
    "            \"  len(dev_fold): {dev}\\n\"\n",
    "            \"  len(test_fold): {test}\\n\"\n",
    "            \"  len(joined): {joined}\\n\"\n",
    "            \"  N: {N}\".format(\n",
    "                N=N, fold=fold, train=len(train_fold), dev=len(dev_fold), test=len(test_fold), joined=len(joined)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        splits.append((\n",
    "            data.iloc[train_fold],\n",
    "            data.iloc[dev_fold],\n",
    "            data.iloc[test_fold],\n",
    "        ))\n",
    "\n",
    "    assert len(splits) == K, \"Too few splits\"\n",
    "\n",
    "    #below has the dependency info with the directionalities\n",
    "    #seed_1_{K}-dep-dir-fold.pkl\n",
    "    with open(res('pickled-files/' + filename1.format(K=K, p=preprocessing_type)), mode='wb') as f: pickle.dump(splits, f)\n",
    "    with open(res('pickled-files/'+filename2.format(K=K, p=preprocessing_type)), mode='wb') as f: pickle.dump(splits, f, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed is 5\n",
      "seed is 5\n",
      "seed is 5\n",
      "seed is 5\n"
     ]
    }
   ],
   "source": [
    "pickle_data(data_borderminus1, 'seed_5_{K}-fold-border_-1_{p}.pkl', 'seed_5_{K}-fold-border_-1_{p}_py2.pkl', 5, preprocessing_type)\n",
    "pickle_data(data_border1, 'seed_5_{K}-fold-border_1_{p}.pkl', 'seed_5_{K}-fold-border_1_{p}_py2.pkl', 5, preprocessing_type)\n",
    "pickle_data(data_border20, 'seed_5_{K}-fold-border_20_{p}.pkl', 'seed_5_{K}-fold-border_20_{p}_py2.pkl', 5, preprocessing_type)\n",
    "pickle_data(data_border50, 'seed_5_{K}-fold-border_50_{p}.pkl', 'seed_5_{K}-fold-border_50_{p}_py2.pkl', 5, preprocessing_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
