{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods to perform error analysis\n",
    "# Given the result.txt file created by the official script, extract useful data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import os\n",
    "from sys import path\n",
    "import re\n",
    "import pandas as pd\n",
    "#path.append('..')\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "output_path = '/scratch/geeticka/relation-extraction/output/semeval2010/CrossValidation'\n",
    "def res(path): return os.path.join(output_path, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file_location = res('cnn_72aaf050-50a5-42b1-b7d8-3893b12fd708_2018-12-16dataset_semeval2010-pos_embed_size_25-num_filters_100-filter_sizes_2,3,4,5-keep_prob_0.500000-early_stop_False-patience_100/Fold0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below are the methods that gather the necessary information from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_confusion_matrix_per_line(cur_line):\n",
    "    if re.search(r'.*\\|.*', cur_line): # only get those lines which have a pipe operator\n",
    "        splitted_line = cur_line.strip().split()\n",
    "        pipe_seen = 0 # the correct numbers are between two pipes\n",
    "        confusion_matrix_line = []\n",
    "        for val in splitted_line:\n",
    "            if val == '|':\n",
    "                pipe_seen += 1\n",
    "            if pipe_seen == 1 and val != '|': # keep collecting the values as you are\n",
    "                confusion_matrix_line.append(float(val))\n",
    "        return confusion_matrix_line\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_accuracy_per_line(cur_line):\n",
    "    if cur_line.startswith('Accuracy (calculated'):\n",
    "        accuracy = re.match(r'.*= (.*)%', cur_line).groups()[0]\n",
    "        accuracy = float(accuracy)\n",
    "        return accuracy\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_precision_recall_f1(cur_line): # assume that the mode is once we have read 'Results for the individual' \n",
    "    match = re.match(r'.*= (.*)%.*= (.*)%.*= (.*)%$', cur_line)\n",
    "    if match:\n",
    "        precision, recall, f1 = match.groups()\n",
    "        return float(precision), float(recall), float(f1)\n",
    "    else:\n",
    "        return None\n",
    "    #if not cur_line.startswith('Micro-averaged result'): # you want to read only up to the point when the relations\n",
    "    # will need to double check above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_metrics(result_file_location):\n",
    "    result_file = os.path.join(result_file_location, 'result.txt')\n",
    "    official_portion_file = False\n",
    "    individual_relations_f1_portion = False\n",
    "    micro_f1_portion = False\n",
    "    macro_f1_portion = False\n",
    "    confusion_matrix_official = [] # stores the official confusion matrix read from the file\n",
    "    accuracy = None\n",
    "    metrics_indiv_relations = [] # precision, recall and f1 for each relation\n",
    "    metrics_micro = [] # excluding the other relation\n",
    "    metrics_macro = [] # excluding the other relation\n",
    "    with open(result_file, 'r') as result_file:\n",
    "        for cur_line in result_file:\n",
    "            cur_line = cur_line.strip()\n",
    "            if cur_line.startswith('<<< (9+1)-WAY EVALUATION TAKING DIRECTIONALITY INTO ACCOUNT -- OFFICIAL >>>'):\n",
    "                official_portion_file = True\n",
    "            if official_portion_file is False:\n",
    "                continue\n",
    "            confusion_matrix_line = read_confusion_matrix_per_line(cur_line)\n",
    "            if confusion_matrix_line is not None: confusion_matrix_official.append(confusion_matrix_line)\n",
    "            \n",
    "            acc = read_accuracy_per_line(cur_line)\n",
    "            if acc is not None: accuracy = acc\n",
    "            \n",
    "            # figure out which sub portion of the official portion we are in \n",
    "            if cur_line.startswith('Results for the individual relations:'):\n",
    "                individual_relations_f1_portion = True\n",
    "            elif cur_line.startswith('Micro-averaged result (excluding Other):'):\n",
    "                micro_f1_portion = True\n",
    "            elif cur_line.startswith('MACRO-averaged result (excluding Other):'):\n",
    "                macro_f1_portion = True\n",
    "            \n",
    "            # populate the precision, recall and f1 for the correct respective lists\n",
    "            if individual_relations_f1_portion is True and micro_f1_portion is False:\n",
    "                vals = read_precision_recall_f1(cur_line)\n",
    "                if vals is not None: metrics_indiv_relations.append([vals[0], vals[1], vals[2]])\n",
    "            elif micro_f1_portion is True and macro_f1_portion is False:\n",
    "                vals = read_precision_recall_f1(cur_line)\n",
    "                if vals is not None: metrics_micro.append([vals[0], vals[1], vals[2]])\n",
    "            elif macro_f1_portion is True:\n",
    "                vals = read_precision_recall_f1(cur_line)\n",
    "                if vals is not None: metrics_macro.append([vals[0], vals[1], vals[2]])\n",
    "    return confusion_matrix_official, accuracy, metrics_indiv_relations, metrics_micro, metrics_macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the file metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_official, accuracy, \\\n",
    "metrics_indiv_relations, metrics_micro, metrics_macro = get_file_metrics(result_file_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the confusion matrix as a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/17091769/python-pandas-fill-a-dataframe-row-by-row\n",
    "and https://stackoverflow.com/questions/35047842/how-to-store-the-name-of-rows-and-column-index-in-pandas-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_full_form_dictionary = {'C-E': 'Cause-Effect', 'C-W': 'Component-Whole', 'C-C': 'Content-Container',\n",
    "                                 'E-D': 'Entity-Destination', 'E-O': 'Entity-Origin', 'I-A': 'Instrument-Agency',\n",
    "                                 'M-C': 'Member-Collection', 'M-T': 'Message-Topic', 'P-P': 'Product-Producer',\n",
    "                                 '_O': 'Other'}\n",
    "relation_as_short_list = ['C-E', 'C-W', 'C-C', 'E-D', 'E-O', 'I-A', 'M-C', 'M-T', 'P-P', '_O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix_as_df(confusion_matrix_official, relations_as_short_list):\n",
    "    index = pd.Index(relations_as_short_list, name='gold labels')\n",
    "    columns = pd.Index(relations_as_short_list, name='predicted')\n",
    "    confusion_matrix_df = pd.DataFrame(data=confusion_matrix_official, columns=columns,index=index)\n",
    "    return confusion_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_df = get_confusion_matrix_as_df(confusion_matrix_official, relation_as_short_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Give the confusions across each relation, with a special interest on other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to go row by row, and get only those column names which have 0 values.\n",
    "#and the number associated with that column name as a string. \n",
    "# and we also want to get the correct values as a separate column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confused_with_string(index, row, relation_full_form_dictionary):\n",
    "    # index is the current relation that we are considering and row is all the predicted examples\n",
    "    confused_with_string = \"\"\n",
    "    num_of_columns = len(row.index)\n",
    "    for i in range(0, num_of_columns):\n",
    "        column_name = row.index[i]\n",
    "        column_value = int(row.loc[column_name])\n",
    "        if column_value > 0 and column_name != index:\n",
    "            confused_with_string += \" \" + relation_full_form_dictionary[column_name] + \\\n",
    "            \"(\" + str(column_value) + \")\"\n",
    "    return confused_with_string.strip()\n",
    "    #print(row.data[0])\n",
    "    #for val in row:\n",
    "    #    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pretty_summary_confusion_matrix(confusion_matrix_df, relation_full_form_dictionary):\n",
    "    data = [] # index will be 0,1,2 and so on, but columns will be\n",
    "    # Actual label, confused with as a string, correct predictions as a number\n",
    "    for index, row in confusion_matrix_df.iterrows():\n",
    "        actual_label = relation_full_form_dictionary[index]\n",
    "        confused_with = generate_confused_with_string(index, row, relation_full_form_dictionary)\n",
    "        correct_predictions = row[index] # eg: gives the column value for C-E for an index C-E\n",
    "        if index != '_O': confused_with_other = row['_O'] # this is specific to semeval and will need to be changed\n",
    "        else: confused_with_other = None\n",
    "        data.append([actual_label, confused_with, confused_with_other, correct_predictions])\n",
    "    columns = pd.Index(['Gold Relation', 'Confused With(num_examples)', 'Confused with Other', 'Correct Predictions'], name='summary')\n",
    "    pretty_summary_confusion_matrix_df = pd.DataFrame(data=data, columns=columns)\n",
    "    return pretty_summary_confusion_matrix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Give the individual relation metrics as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_indiv_relations_df(metrics_indiv_relations, relation_full_form_dictionary, relation_as_short_list):\n",
    "    index_list = relation_as_short_list\n",
    "    index_list_verbose = [relation_full_form_dictionary[x] for x in index_list]\n",
    "    index = pd.Index(index_list_verbose, name='labels')\n",
    "    columns = pd.Index(['Precision', 'Recall', 'F1'], name='metrics')\n",
    "    metrics_indiv_relations_df = pd.DataFrame(data=metrics_indiv_relations, columns=columns,index=index)\n",
    "    return metrics_indiv_relations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_macro_micro_df(metrics_macro, metrics_micro):\n",
    "    data = metrics_macro + metrics_micro\n",
    "    index = pd.Index(['macro', 'micro'], name='calculation type')\n",
    "    columns = pd.Index(['Precision', 'Recall', 'F1'], name='metrics')\n",
    "    metrics_macro_micro = pd.DataFrame(data=data, columns=columns,index=index)\n",
    "    return metrics_macro_micro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, create a large summary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_full_form_dictionary = {'C-E': 'Cause-Effect', 'C-W': 'Component-Whole', 'C-C': 'Content-Container',\n",
    "                                 'E-D': 'Entity-Destination', 'E-O': 'Entity-Origin', 'I-A': 'Instrument-Agency',\n",
    "                                 'M-C': 'Member-Collection', 'M-T': 'Message-Topic', 'P-P': 'Product-Producer',\n",
    "                                 '_O': 'Other'}\n",
    "relation_as_short_list = ['C-E', 'C-W', 'C-C', 'E-D', 'E-O', 'I-A', 'M-C', 'M-T', 'P-P', '_O'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary(result_file_location, relation_full_form_dictionary, relation_as_short_list):\n",
    "    if not os.path.exists(result_file_location):\n",
    "        print(\"Check your path first!\")\n",
    "        return None\n",
    "    # get the file metrics\n",
    "    confusion_matrix_official, accuracy, \\\n",
    "    metrics_indiv_relations, metrics_micro, metrics_macro = get_file_metrics(result_file_location)\n",
    "    \n",
    "    # get the confusion matrix dataframe\n",
    "    confusion_matrix_df = get_confusion_matrix_as_df(confusion_matrix_official, relation_as_short_list)\n",
    "    \n",
    "    # these are the summary information that will need to be returned\n",
    "    pretty_summary_confusion_matrix_df = generate_pretty_summary_confusion_matrix(confusion_matrix_df, \n",
    "                                                                                  relation_full_form_dictionary)\n",
    "    total_correct_predictions = pretty_summary_confusion_matrix_df['Correct Predictions'].sum()\n",
    "    metrics_indiv_relations_df = create_metrics_indiv_relations_df(metrics_indiv_relations, \n",
    "                                                                   relation_full_form_dictionary, \n",
    "                                                                   relation_as_short_list)\n",
    "    metrics_macro_micro = create_metrics_macro_micro_df(metrics_macro, metrics_micro)\n",
    "    # report accuracy as well\n",
    "    return confusion_matrix_df, pretty_summary_confusion_matrix_df, total_correct_predictions, metrics_indiv_relations_df, \\\n",
    "    metrics_macro_micro, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_df, pretty_summary_confusion_matrix_df, \\\n",
    "total_correct_predictions, metrics_indiv_relations_df, \\\n",
    "metrics_macro_micro, accuracy = create_summary(result_file_location, relation_full_form_dictionary, relation_as_short_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the confusion matrix, return the sums of all the examples\n",
    "def get_sum_confusion_matrix(confusion_matrix):\n",
    "    sum = 0\n",
    "    for column in confusion_matrix:\n",
    "        sum += confusion_matrix[column].sum()\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of the relations, do a t test between the two model metrics\n",
    "def indiv_metric_comparison(metrics_i_model1, metrics_i_model2, model1_name, model2_name):\n",
    "    print(\"TTest from %s to %s\"%(model1_name, model2_name))\n",
    "    print(\"Below is the metric comparsion across the two models\" + \\\n",
    "          \"considering individual relations, excluding 'Other'\")\n",
    "    for column in metrics_i_model1:\n",
    "        metric_model1 = metrics_i_model1[column].tolist()[:-1] # excluding \"Other\"\n",
    "        metric_model2 = metrics_i_model2[column].tolist()[:-1]\n",
    "        tt = ttest_rel(metric_model1, metric_model2)\n",
    "        print(\"Metric: %s \\t statistic %.2f \\t p_value %s\"%\n",
    "              (column, tt.statistic, tt.pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_macro_micro_metric_comparison(metrics_ma_mi_model1, metrics_ma_mi_model2, model1_name, model2_name):\n",
    "    print(\"Macro - Micro for the %s model\"%(model1_name))\n",
    "    for column in metrics_ma_mi_model1:\n",
    "        macro = metrics_ma_mi_model1[column].loc['macro']\n",
    "        micro = metrics_ma_mi_model1[column].loc['micro']\n",
    "        print(\"Metric: %s \\t Macro-Micro %.2f\"%(column, macro-micro))\n",
    "        \n",
    "    print(\"\\nMacro - Micro for the %s model\"%(model2_name))\n",
    "    for column in metrics_ma_mi_model2:\n",
    "        macro = metrics_ma_mi_model2[column].loc['macro']\n",
    "        micro = metrics_ma_mi_model2[column].loc['micro']\n",
    "        print(\"Metric: %s \\t Macro-Micro %.2f\"%(column, macro-micro))\n",
    "        \n",
    "    print(\"\\nMacro_%s - Macro_%s\"%(model1_name, model2_name))\n",
    "    for column in metrics_ma_mi_model1:\n",
    "        macro_model1 = metrics_ma_mi_model1[column].loc['macro']\n",
    "        macro_model2 = metrics_ma_mi_model2[column].loc['macro']\n",
    "        print(\"Metric: %s \\t Difference %.2f\"%(column, macro_model1-macro_model2))\n",
    "    \n",
    "    print(\"\\nMicro_%s - Micro_%s\"%(model1_name, model2_name))\n",
    "    for column in metrics_ma_mi_model1:\n",
    "        micro_model1 = metrics_ma_mi_model1[column].loc['micro']\n",
    "        micro_model2 = metrics_ma_mi_model2[column].loc['micro']\n",
    "        print(\"Metric: %s \\t Difference %.2f\"%(column, micro_model1-micro_model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_difference(accuracy_model1, accuracy_model2, model1_name, model2_name):\n",
    "    print(\"Accuracy_%s - Accuracy_%s %.2f\"%(model1_name, model2_name, accuracy_model1 - accuracy_model2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In nightingale, I created 2 folders inside \n",
    "/scratch/geeticka/relation-extraction/output/semeval2010/CrossValidation/error-analysis\n",
    "\n",
    "- One folder is baseline, the other is elmo-model\n",
    "- Each folder has result.txt for each Fold\n",
    "- I am going to generate a summary for each of the Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full_summary(model1_loc, model2_loc, model1_name, model2_name, res, \n",
    "                       relation_full_form_dictionary, relation_as_short_list):\n",
    "    model1 = res(model1_loc)\n",
    "    model2 = res(model2_loc)\n",
    "    \n",
    "    cm_model1, summary_cm_model1, correct_pred_model1, metrics_i_model1, \\\n",
    "    metrics_ma_mi_model1, accuracy_model1 \\\n",
    "    = create_summary(model1, relation_full_form_dictionary, relation_as_short_list)\n",
    "    \n",
    "    cm_model2, summary_cm_model2, correct_pred_model2, metrics_i_model2, \\\n",
    "    metrics_ma_mi_model2, accuracy_model2 \\\n",
    "    = create_summary(model2, relation_full_form_dictionary, relation_as_short_list)\n",
    "    \n",
    "    # T Test of each metrics, across the relations not Other\n",
    "    indiv_metric_comparison(metrics_i_model1, metrics_i_model2, model1_name, model2_name)\n",
    "    \n",
    "    # Get the difference in the macro and micro scores\n",
    "    get_macro_micro_metric_comparison(metrics_ma_mi_model1, metrics_ma_mi_model2, model1_name, model2_name)\n",
    "    \n",
    "    # Print the accuracy difference as well\n",
    "    get_accuracy_difference(accuracy_model1, accuracy_model2, model1_name, model2_name)\n",
    "    return summary_cm_model1, summary_cm_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTest from Baseline to Elmo\n",
      "Below is the metric comparsion across the two modelsconsidering individual relations, excluding 'Other'\n",
      "Metric: Precision \t statistic -5.38 \t p_value 0.0006647655060868683\n",
      "Metric: Recall \t statistic 0.45 \t p_value 0.6644537071409055\n",
      "Metric: F1 \t statistic -3.34 \t p_value 0.010214137647466863\n",
      "Macro - Micro for the Baseline model\n",
      "Metric: Precision \t Macro-Micro 0.37\n",
      "Metric: Recall \t Macro-Micro -0.77\n",
      "Metric: F1 \t Macro-Micro -0.47\n",
      "\n",
      "Macro - Micro for the Elmo model\n",
      "Metric: Precision \t Macro-Micro 0.71\n",
      "Metric: Recall \t Macro-Micro -1.07\n",
      "Metric: F1 \t Macro-Micro -0.64\n",
      "\n",
      "Macro_Baseline - Macro_Elmo\n",
      "Metric: Precision \t Difference -7.46\n",
      "Metric: Recall \t Difference 0.75\n",
      "Metric: F1 \t Difference -3.21\n",
      "\n",
      "Micro_Baseline - Micro_Elmo\n",
      "Metric: Precision \t Difference -7.12\n",
      "Metric: Recall \t Difference 0.45\n",
      "Metric: F1 \t Difference -3.38\n",
      "Accuracy_Baseline - Accuracy_Elmo -3.23\n"
     ]
    }
   ],
   "source": [
    "summary_cm_baseline, summary_cm_elmo = print_full_summary(\n",
    "    'error-analysis/baseline/Fold0', 'error-analysis/elmo-model/Fold0', \n",
    "    'Baseline','Elmo', res, relation_full_form_dictionary, relation_as_short_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>summary</th>\n",
       "      <th>Gold Relation</th>\n",
       "      <th>Confused With(num_examples)</th>\n",
       "      <th>Confused with Other</th>\n",
       "      <th>Correct Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cause-Effect</td>\n",
       "      <td>Entity-Origin(6) Message-Topic(1) Product-Prod...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Component-Whole</td>\n",
       "      <td>Content-Container(1) Entity-Origin(2) Member-C...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Content-Container</td>\n",
       "      <td>Component-Whole(2) Entity-Destination(4) Other(1)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Entity-Destination</td>\n",
       "      <td>Content-Container(1)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Entity-Origin</td>\n",
       "      <td>Component-Whole(1) Other(4)</td>\n",
       "      <td>4.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Instrument-Agency</td>\n",
       "      <td>Component-Whole(5) Entity-Origin(1) Message-To...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Member-Collection</td>\n",
       "      <td>Component-Whole(2) Content-Container(2) Other(2)</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Message-Topic</td>\n",
       "      <td>Component-Whole(1) Entity-Destination(1) Produ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Product-Producer</td>\n",
       "      <td>Cause-Effect(1) Entity-Destination(1) Entity-O...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Other</td>\n",
       "      <td>Cause-Effect(10) Component-Whole(15) Content-C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "summary       Gold Relation  \\\n",
       "0              Cause-Effect   \n",
       "1           Component-Whole   \n",
       "2         Content-Container   \n",
       "3        Entity-Destination   \n",
       "4             Entity-Origin   \n",
       "5         Instrument-Agency   \n",
       "6         Member-Collection   \n",
       "7             Message-Topic   \n",
       "8          Product-Producer   \n",
       "9                     Other   \n",
       "\n",
       "summary                        Confused With(num_examples)  \\\n",
       "0        Entity-Origin(6) Message-Topic(1) Product-Prod...   \n",
       "1        Content-Container(1) Entity-Origin(2) Member-C...   \n",
       "2        Component-Whole(2) Entity-Destination(4) Other(1)   \n",
       "3                                     Content-Container(1)   \n",
       "4                              Component-Whole(1) Other(4)   \n",
       "5        Component-Whole(5) Entity-Origin(1) Message-To...   \n",
       "6         Component-Whole(2) Content-Container(2) Other(2)   \n",
       "7        Component-Whole(1) Entity-Destination(1) Produ...   \n",
       "8        Cause-Effect(1) Entity-Destination(1) Entity-O...   \n",
       "9        Cause-Effect(10) Component-Whole(15) Content-C...   \n",
       "\n",
       "summary  Confused with Other  Correct Predictions  \n",
       "0                        2.0                 91.0  \n",
       "1                       12.0                 71.0  \n",
       "2                        1.0                 48.0  \n",
       "3                        0.0                 84.0  \n",
       "4                        4.0                 67.0  \n",
       "5                        6.0                 34.0  \n",
       "6                        2.0                 64.0  \n",
       "7                        8.0                 50.0  \n",
       "8                        9.0                 57.0  \n",
       "9                        NaN                 56.0  "
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_cm_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>summary</th>\n",
       "      <th>Gold Relation</th>\n",
       "      <th>Confused With(num_examples)</th>\n",
       "      <th>Confused with Other</th>\n",
       "      <th>Correct Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cause-Effect</td>\n",
       "      <td>Entity-Origin(3) Product-Producer(1) Other(2)</td>\n",
       "      <td>2.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Component-Whole</td>\n",
       "      <td>Member-Collection(2) Message-Topic(2) Other(21)</td>\n",
       "      <td>21.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Content-Container</td>\n",
       "      <td>Entity-Destination(6) Other(6)</td>\n",
       "      <td>6.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Entity-Destination</td>\n",
       "      <td>Other(2)</td>\n",
       "      <td>2.0</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Entity-Origin</td>\n",
       "      <td>Component-Whole(1) Other(6)</td>\n",
       "      <td>6.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Instrument-Agency</td>\n",
       "      <td>Component-Whole(3) Message-Topic(1) Other(12)</td>\n",
       "      <td>12.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Member-Collection</td>\n",
       "      <td>Component-Whole(2) Other(7)</td>\n",
       "      <td>7.0</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Message-Topic</td>\n",
       "      <td>Other(13)</td>\n",
       "      <td>13.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Product-Producer</td>\n",
       "      <td>Entity-Origin(3) Other(7)</td>\n",
       "      <td>7.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Other</td>\n",
       "      <td>Cause-Effect(8) Component-Whole(8) Content-Con...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "summary       Gold Relation  \\\n",
       "0              Cause-Effect   \n",
       "1           Component-Whole   \n",
       "2         Content-Container   \n",
       "3        Entity-Destination   \n",
       "4             Entity-Origin   \n",
       "5         Instrument-Agency   \n",
       "6         Member-Collection   \n",
       "7             Message-Topic   \n",
       "8          Product-Producer   \n",
       "9                     Other   \n",
       "\n",
       "summary                        Confused With(num_examples)  \\\n",
       "0            Entity-Origin(3) Product-Producer(1) Other(2)   \n",
       "1          Member-Collection(2) Message-Topic(2) Other(21)   \n",
       "2                           Entity-Destination(6) Other(6)   \n",
       "3                                                 Other(2)   \n",
       "4                              Component-Whole(1) Other(6)   \n",
       "5            Component-Whole(3) Message-Topic(1) Other(12)   \n",
       "6                              Component-Whole(2) Other(7)   \n",
       "7                                                Other(13)   \n",
       "8                                Entity-Origin(3) Other(7)   \n",
       "9        Cause-Effect(8) Component-Whole(8) Content-Con...   \n",
       "\n",
       "summary  Confused with Other  Correct Predictions  \n",
       "0                        2.0                 95.0  \n",
       "1                       21.0                 68.0  \n",
       "2                        6.0                 43.0  \n",
       "3                        2.0                 83.0  \n",
       "4                        6.0                 65.0  \n",
       "5                       12.0                 34.0  \n",
       "6                        7.0                 61.0  \n",
       "7                       13.0                 51.0  \n",
       "8                        7.0                 63.0  \n",
       "9                        NaN                 85.0  "
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_cm_elmo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
